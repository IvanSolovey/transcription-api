"""
–ì–ª–æ–±–∞–ª—å–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π Whisper –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º'—è—Ç—ñ.

–ó–∞–±–µ–∑–ø–µ—á—É—î:
- –Ñ–¥–∏–Ω–∏–π —ñ–Ω—Å—Ç–∞–Ω—Å –º–æ–¥–µ–ª—ñ –≤ –ø–∞–º'—è—Ç—ñ (singleton)
- –ë–µ–∑–ø–µ—á–Ω–µ –ø–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –º—ñ–∂ —Ä–æ–∑–º—ñ—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π
- –ë–ª–æ–∫—É–≤–∞–Ω–Ω—è –ø—ñ–¥ —á–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è/–≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
- –ü–µ—Ä–µ–≤—ñ—Ä–∫—É –¥–æ—Å—Ç—É–ø–Ω–æ—ó –ø–∞–º'—è—Ç—ñ –ø–µ—Ä–µ–¥ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º
"""

import os
import gc
import logging
import threading
from typing import Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# –ü—Ä–∏–±–ª–∏–∑–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª–µ–π —É RAM (INT8 quantized, GB)
MODEL_MEMORY_REQUIREMENTS = {
    "tiny": 0.5,
    "base": 0.8,
    "small": 1.2,
    "medium": 2.5,
    "large": 4.5,
    "large-v2": 4.5,
    "large-v3": 4.5,
}

# –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –∑–∞–ø–∞—Å –ø–∞–º'—è—Ç—ñ –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ—ó —Ä–æ–±–æ—Ç–∏ (GB)
# –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –º–∞—à–∏–Ω –∑ –æ–±–º–µ–∂–µ–Ω–æ—é RAM
MEMORY_SAFETY_MARGIN_GB = 0.5

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–∞–º'—è—Ç—ñ: True = —Å—Ç—Ä–æ–≥–∞ (–≤—ñ–¥—Ö–∏–ª—è—Ç–∏ —è–∫—â–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ), False = –º'—è–∫–∞ (—Ç—ñ–ª—å–∫–∏ warning)
STRICT_MEMORY_CHECK = os.environ.get("STRICT_MEMORY_CHECK", "false").lower() == "true"


@dataclass
class ModelInfo:
    """–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—É –º–æ–¥–µ–ª—å"""
    model_size: str
    device: str
    compute_type: str
    loaded_at: float
    memory_usage_gb: float


class ModelManager:
    """
    Singleton –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è Whisper –º–æ–¥–µ–ª–µ–π.
    
    –ì–∞—Ä–∞–Ω—Ç—É—î, —â–æ –≤ –ø–∞–º'—è—Ç—ñ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –ª–∏—à–µ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å,
    —ñ –±–µ–∑–ø–µ—á–Ω–æ –ø–µ—Ä–µ–º–∏–∫–∞—î –º—ñ–∂ —Ä–æ–∑–º—ñ—Ä–∞–º–∏.
    """
    
    _instance = None
    _lock = threading.RLock()  # Reentrant lock –¥–ª—è –≤–∫–ª–∞–¥–µ–Ω–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        self._model = None
        self._model_info: Optional[ModelInfo] = None
        self._loading = False
        self._initialized = True
        logger.info("üîß ModelManager —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ (singleton)")
    
    @property
    def current_model(self):
        """–ü–æ—Ç–æ—á–Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å"""
        return self._model
    
    @property
    def current_model_size(self) -> Optional[str]:
        """–†–æ–∑–º—ñ—Ä –ø–æ—Ç–æ—á–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
        return self._model_info.model_size if self._model_info else None
    
    @property
    def is_loading(self) -> bool:
        """–ß–∏ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –∑–∞—Ä–∞–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"""
        return self._loading
    
    def get_available_memory_gb(self) -> float:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –¥–æ—Å—Ç—É–ø–Ω—É –ø–∞–º'—è—Ç—å —É GB"""
        try:
            import psutil
            mem = psutil.virtual_memory()
            available_gb = mem.available / (1024 ** 3)
            return available_gb
        except ImportError:
            logger.warning("psutil –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ 8GB –≤—ñ–ª—å–Ω–æ—ó –ø–∞–º'—è—Ç—ñ")
            return 8.0
    
    def get_total_memory_gb(self) -> float:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –∑–∞–≥–∞–ª—å–Ω—É –ø–∞–º'—è—Ç—å —É GB"""
        try:
            import psutil
            mem = psutil.virtual_memory()
            return mem.total / (1024 ** 3)
        except ImportError:
            return 16.0
    
    def can_load_model(self, model_size: str, strict: bool = None) -> tuple[bool, str]:
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –º–æ–∂–Ω–∞ –±–µ–∑–ø–µ—á–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å.
        
        Args:
            model_size: –†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ
            strict: True = –±–ª–æ–∫—É–≤–∞—Ç–∏ —è–∫—â–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ, False = —Ç—ñ–ª—å–∫–∏ warning, None = –∑–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º
        
        Returns:
            (can_load, reason)
        """
        if strict is None:
            strict = STRICT_MEMORY_CHECK
            
        required_memory = MODEL_MEMORY_REQUIREMENTS.get(model_size, 2.0)
        available_memory = self.get_available_memory_gb()
        total_memory = self.get_total_memory_gb()
        
        # –Ø–∫—â–æ –º–æ–¥–µ–ª—å —Ç–æ–≥–æ –∂ —Ä–æ–∑–º—ñ—Ä—É –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ - OK
        if self._model and self._model_info and self._model_info.model_size == model_size:
            return True, "Model already loaded"
        
        # –í—Ä–∞—Ö–æ–≤—É—î–º–æ, —â–æ —Å—Ç–∞—Ä–∞ –º–æ–¥–µ–ª—å –±—É–¥–µ –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞
        current_model_memory = 0
        if self._model_info:
            current_model_memory = MODEL_MEMORY_REQUIREMENTS.get(
                self._model_info.model_size, 0
            )
        
        # –î–æ—Å—Ç—É–ø–Ω–∞ –ø–∞–º'—è—Ç—å –ø—ñ—Å–ª—è –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–∞—Ä–æ—ó –º–æ–¥–µ–ª—ñ
        effective_available = available_memory + current_model_memory
        needed = required_memory + MEMORY_SAFETY_MARGIN_GB
        
        if effective_available < needed:
            reason = (
                f"Insufficient memory: need {needed:.1f}GB, "
                f"available {effective_available:.1f}GB (total {total_memory:.1f}GB)"
            )
            if strict:
                return False, reason
            else:
                # –ú'—è–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ - —Ç—ñ–ª—å–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è, –∞–ª–µ –¥–æ–∑–≤–æ–ª—è—î–º–æ —Å–ø—Ä–æ–±—É
                logger.warning(f"‚ö†Ô∏è {reason} - attempting anyway (STRICT_MEMORY_CHECK=false)")
                return True, f"Warning: {reason}"
        
        return True, "OK"
    
    def unload_model(self) -> bool:
        """
        –í–∏–≤–∞–Ω—Ç–∞–∂—É—î –ø–æ—Ç–æ—á–Ω—É –º–æ–¥–µ–ª—å –∑ –ø–∞–º'—è—Ç—ñ.
        
        Returns:
            True —è–∫—â–æ –º–æ–¥–µ–ª—å –±—É–ª–∞ –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞, False —è–∫—â–æ –º–æ–¥–µ–ª—ñ –Ω–µ –±—É–ª–æ
        """
        with self._lock:
            if self._model is None:
                logger.debug("–ù–µ–º–∞—î –º–æ–¥–µ–ª—ñ –¥–ª—è –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è")
                return False
            
            old_size = self._model_info.model_size if self._model_info else "unknown"
            logger.info(f"üóëÔ∏è –í–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {old_size}...")
            
            # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –º–æ–¥–µ–ª—å
            self._model = None
            self._model_info = None
            
            # –ê–≥—Ä–µ—Å–∏–≤–Ω–µ –æ—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
            for _ in range(5):
                gc.collect()
            
            # –°–ø—Ä–æ–±–∞ –æ—á–∏—Å—Ç–∏—Ç–∏ CUDA –∫–µ—à —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            except Exception:
                pass
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {old_size} –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞, RAM: {self.get_available_memory_gb():.1f}GB –≤—ñ–ª—å–Ω–æ")
            return True
    
    def load_model(self, model_size: str, device: str = "cpu", force: bool = False) -> Optional[Any]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å Whisper, –≤–∏–≤–∞–Ω—Ç–∞–∂—É—é—á–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—é —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ.
        
        Args:
            model_size: –†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ (tiny, base, small, medium, large)
            device: –ü—Ä–∏—Å—Ç—Ä—ñ–π (cpu/cuda)
            force: –ü—Ä–∏–º—É—Å–æ–≤–æ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ —Ç–∞ —Å–∞–º–∞ –º–æ–¥–µ–ª—å
        
        Returns:
            –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∞–±–æ None –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
        """
        with self._lock:
            # –Ø–∫—â–æ –º–æ–¥–µ–ª—å —Ç–æ–≥–æ –∂ —Ä–æ–∑–º—ñ—Ä—É –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —ó—ó
            if not force and self._model and self._model_info:
                if self._model_info.model_size == model_size:
                    logger.debug(f"–ú–æ–¥–µ–ª—å {model_size} –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
                    return self._model
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –º–æ–∂–Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏
            can_load, reason = self.can_load_model(model_size)
            if not can_load:
                logger.error(f"‚ùå –ù–µ–º–æ–∂–ª–∏–≤–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å {model_size}: {reason}")
                raise MemoryError(f"Cannot load model {model_size}: {reason}")
            
            self._loading = True
            
            try:
                # –í–∏–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–∞—Ä—É –º–æ–¥–µ–ª—å
                if self._model is not None:
                    old_size = self._model_info.model_size if self._model_info else "unknown"
                    logger.info(f"üîÑ –ü–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –∑ {old_size} –Ω–∞ {model_size}")
                    self.unload_model()
                
                # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—É –º–æ–¥–µ–ª—å
                logger.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_size}...")
                
                from faster_whisper import WhisperModel
                from .config import MODELS_DIR, CPU_COMPUTE_TYPE, GPU_COMPUTE_TYPE
                
                compute_type = CPU_COMPUTE_TYPE if device == "cpu" else GPU_COMPUTE_TYPE
                cpu_threads = min(8, os.cpu_count() or 4)
                
                import time
                start_time = time.time()
                
                self._model = WhisperModel(
                    model_size,
                    device=device,
                    compute_type=compute_type,
                    cpu_threads=cpu_threads,
                    num_workers=2 if device == "cpu" else 1,
                    download_root=str(MODELS_DIR)
                )
                
                load_time = time.time() - start_time
                memory_used = MODEL_MEMORY_REQUIREMENTS.get(model_size, 2.0)
                
                self._model_info = ModelInfo(
                    model_size=model_size,
                    device=device,
                    compute_type=compute_type,
                    loaded_at=time.time(),
                    memory_usage_gb=memory_used
                )
                
                logger.info(
                    f"‚úÖ –ú–æ–¥–µ–ª—å {model_size} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑–∞ {load_time:.1f}—Å "
                    f"(~{memory_used:.1f}GB RAM, –≤—ñ–ª—å–Ω–æ: {self.get_available_memory_gb():.1f}GB)"
                )
                
                return self._model
                
            except MemoryError:
                raise
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_size}: {e}")
                self._model = None
                self._model_info = None
                raise
            finally:
                self._loading = False
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç—É—Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
        return {
            "model_loaded": self._model is not None,
            "current_model_size": self._model_info.model_size if self._model_info else None,
            "current_device": self._model_info.device if self._model_info else None,
            "is_loading": self._loading,
            "available_memory_gb": round(self.get_available_memory_gb(), 2),
            "total_memory_gb": round(self.get_total_memory_gb(), 2),
            "model_memory_requirements": MODEL_MEMORY_REQUIREMENTS,
        }


# –ì–ª–æ–±–∞–ª—å–Ω–∏–π singleton —ñ–Ω—Å—Ç–∞–Ω—Å
model_manager = ModelManager()
